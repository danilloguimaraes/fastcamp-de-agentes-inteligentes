{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "7Yw84kvZ1NHc",
        "DrbjTyHmzD_O",
        "8NvJWtBYzZb6"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Configuração e teste\n",
        "Aqui o script instala o ferramental necessário para o experimento.\n",
        "\n",
        "\n",
        "1.   Acesso a api-key do grok (foi a llm utilizada no vídeo)\n",
        "2.   Instalação da dependência da api do groq. (Ela utilizará a api-key pra se comunicar com os servidores do grok)\n",
        "\n",
        "Depois é realizado uma interação com a api do grok"
      ],
      "metadata": {
        "id": "7Yw84kvZ1NHc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install groq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0vsEzfUyG0z",
        "outputId": "e7e2f363-f340-4d88-8008-ff18ad4441e3"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: groq in /usr/local/lib/python3.12/dist-packages (1.0.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from groq) (4.12.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from groq) (2.12.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.12/dist-packages (from groq) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->groq) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.4.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "mHZJ01yay23x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cbbd6b5-ad30-47e8-90c4-c0f71ddfc634"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fast language models are crucial in today's technology landscape, and their importance can be understood from several perspectives:\n",
            "\n",
            "1. **Efficient Processing**: Language models are computationally intensive, requiring significant resources to process and generate text. Fast language models enable efficient processing, reducing the time and computational resources required to complete tasks. This efficiency is critical in applications where real-time responses are essential, such as virtual assistants, chatbots, and language translation systems.\n",
            "2. **Real-time Interactions**: Fast language models facilitate real-time interactions between humans and machines. They enable instant responses to user queries, making conversations more natural and engaging. This is particularly important in applications like customer service, where quick and accurate responses can significantly improve user experience.\n",
            "3. **Scalability**: Fast language models can handle a large volume of requests, making them ideal for applications that require scalable processing. This is essential for large-scale language processing tasks, such as text analysis, sentiment analysis, and language translation.\n",
            "4. **Energy Efficiency**: Fast language models can lead to significant energy savings, as they require less computational power to process text. This is crucial in the context of climate change, where reducing energy consumption and carbon emissions is becoming increasingly important.\n",
            "5. **Improved User Experience**: Fast language models can significantly improve user experience by providing quick and accurate responses. This is particularly important in applications like language translation, where slow response times can lead to frustration and decreased user engagement.\n",
            "6. **Competitive Advantage**: Organizations that adopt fast language models can gain a competitive advantage over those that use slower models. Fast language models can enable businesses to respond quickly to customer inquiries, improve customer satisfaction, and gain a competitive edge in the market.\n",
            "7. **Enabling New Applications**: Fast language models can enable new applications that were previously impossible or impractical. For example, fast language models can enable real-time language translation, sentiment analysis, and text summarization, which can be used in a variety of industries, including healthcare, finance, and education.\n",
            "8. **Research and Development**: Fast language models can accelerate research and development in natural language processing (NLP) and related fields. They enable researchers to test and validate new ideas quickly, leading to faster innovation and progress in the field.\n",
            "\n",
            "Some examples of fast language models include:\n",
            "\n",
            "* **Transformers**: Transformers are a type of neural network architecture that can process text quickly and efficiently. They have become a standard component of many state-of-the-art language models.\n",
            "* **BERT**: BERT (Bidirectional Encoder Representations from Transformers) is a pre-trained language model that can be fine-tuned for specific tasks. It has achieved state-of-the-art results in many NLP tasks and is widely used in industry and academia.\n",
            "* **Long Short-Term Memory (LSTM) Networks**: LSTM networks are a type of recurrent neural network that can process text quickly and efficiently. They are widely used in many NLP applications, including language modeling, text classification, and machine translation.\n",
            "\n",
            "In summary, fast language models are essential for many applications, including real-time interactions, efficient processing, and improved user experience. They can also enable new applications, accelerate research and development, and provide a competitive advantage for businesses that adopt them.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "from google.colab import userdata\n",
        "from groq import Groq\n",
        "\n",
        "llm_api_key = userdata.get('grok_api_key')\n",
        "\n",
        "client = Groq(\n",
        "    api_key = llm_api_key,\n",
        ")\n",
        "\n",
        "chat_completion = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Explain the importance of fast language models\",\n",
        "        }\n",
        "    ],\n",
        "    model=\"llama-3.3-70b-versatile\",\n",
        ")\n",
        "\n",
        "print(chat_completion.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementação do agente e do System Prompt\n",
        "\n",
        "Aqui mantivemos o mesmo agente do experimento original.\n",
        "\n",
        "O agente é definido com a estrutura de construção, inicializando seu estado, e uma function call para registrar as entradas na pilha de interação/conversa. Adicionalmente, há um método execute que dispara a chamada com pilha de conversas para a api de chat completion da llm, no caso o grok."
      ],
      "metadata": {
        "id": "BVlTe09G1USe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "class Agent:\n",
        "\n",
        "  def __init__(self, client, system):\n",
        "    self.client = client\n",
        "    self.system = system\n",
        "    self.messages = []\n",
        "    if self.system is not None:\n",
        "      self.messages.append({\"role\": \"system\", \"content\": self.system})\n",
        "\n",
        "\n",
        "  def __call__(self, message = \"\"):\n",
        "    if message is not None:\n",
        "      self.messages.append({\"role\": \"user\", \"content\": message})\n",
        "\n",
        "    result = self.execute()\n",
        "\n",
        "    self.messages.append({\"role\": \"assistant\", \"content\": result})\n",
        "    return result\n",
        "\n",
        "\n",
        "  def execute(self):\n",
        "    completion = self.client.chat.completions.create(\n",
        "      messages=self.messages,\n",
        "      model=\"llama-3.3-70b-versatile\",\n",
        "    )\n",
        "    return completion.choices[0].message.content\n",
        "\n",
        "\n",
        "  def loop(self, max_iterations=10, query: str = \"\"):\n",
        "\n",
        "    available_tools = {\n",
        "        \"get_menu\": get_menu,\n",
        "        \"calculate\": calculate\n",
        "    }\n",
        "\n",
        "    i = 0\n",
        "\n",
        "    next_prompt = query\n",
        "\n",
        "    while i < max_iterations:\n",
        "        i += 1\n",
        "        result = self.__call__(next_prompt)\n",
        "        print(\"\")\n",
        "        print(f\"--- Iteração {i} ---\")\n",
        "        print(result)\n",
        "\n",
        "        if \"PAUSE\" in result and \"Action\" in result:\n",
        "            # REGRA REGEX:\n",
        "            # 1. ([a-z_]+) -> Nome da ferramenta\n",
        "            # 2. (?::\\s*(.+))? -> Opcional. Procura ':' seguido de espaço e o argumento\n",
        "            match = re.search(r\"Action: ([a-z_]+)(?::\\s*(.+))?\", result, re.IGNORECASE)\n",
        "\n",
        "            if match:\n",
        "                chosen_tool = match.group(1)\n",
        "                arg = match.group(2)\n",
        "\n",
        "                if chosen_tool in available_tools:\n",
        "\n",
        "                    if chosen_tool == \"get_menu\":\n",
        "                      result_tool = available_tools[chosen_tool]()\n",
        "\n",
        "                      # Ou chamando por:\n",
        "                      # result_tool = get_menu()\n",
        "\n",
        "                    else:\n",
        "                      result_tool = available_tools[chosen_tool](arg)\n",
        "\n",
        "                      # Ou chamando por:\n",
        "                      # result_tool = calculate(arg)\n",
        "\n",
        "                    next_prompt = f\"Observation: {result_tool}\"\n",
        "                else:\n",
        "                    next_prompt = \"Observation: Tool not found\"\n",
        "\n",
        "                print(next_prompt)\n",
        "                continue\n",
        "            else:\n",
        "                print(\"Error: Could not parse Action from agent response.\")\n",
        "                break\n",
        "\n",
        "        if \"Answer\" in result:\n",
        "            break\n"
      ],
      "metadata": {
        "id": "RTEuUtbD1YSL"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Definição do system prompt\n",
        "O system prompt foi modificado para atender a regra do experimento atual. Basicamente é um atendente de uma pizzaria que utiliza duas tools (get_menu e calculate)."
      ],
      "metadata": {
        "id": "DrbjTyHmzD_O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = \"\"\"\n",
        "You run in a loop of Thought, Action, PAUSE, Observation.\n",
        "At the end of the loop you output an Answer.\n",
        "Use Thought to describe your thoughts about the question you have been asked.\n",
        "Use Action to run one of the actions available to you - then return PAUSE.\n",
        "Observation will be the result of running those actions.\n",
        "\n",
        "Your goal is to act as a cashier for a Pizza place.\n",
        "Rules:\n",
        "1. You must always retrieve the current menu prices before calculating.\n",
        "2. If the customer eats at the restaurant (dine-in), you MUST add a 10% service fee to the total.\n",
        "3. If the order is for takeout/delivery, there is NO service fee.\n",
        "4. Final Answer must state the items ordered and the final total price.\n",
        "\n",
        "Your available actions are:\n",
        "\n",
        "get_menu:\n",
        "e.g. get_menu\n",
        "Returns a JSON object containing the available pizzas and sodas with their respective prices.\n",
        "\n",
        "calculate:\n",
        "e.g. calculate: 12 + 3\n",
        "Runs a calculation and returns the number. Use Python syntax.\n",
        "\n",
        "Example session 1:\n",
        "\n",
        "Question: I want one Pepperoni pizza and a Coke to go.\n",
        "Thought: I need to check the prices for Pepperoni and Coke.\n",
        "Action: get_menu\n",
        "PAUSE\n",
        "\n",
        "You will be called again with this:\n",
        "\n",
        "Observation: {\"pizzas\": {\"Pepperoni\": 15.00, \"Cheese\": 12.00}, \"sodas\": {\"Coke\": 3.00, \"Sprite\": 3.00}}\n",
        "\n",
        "Thought: Pepperoni is 15.00. Coke is 3.00. The customer said \"to go\", so there is no service fee. I need to sum the prices.\n",
        "Action: calculate: 15 + 3\n",
        "PAUSE\n",
        "\n",
        "You will be called again with this:\n",
        "\n",
        "Observation: 18.0\n",
        "\n",
        "Thought: The calculation is complete. I have the final total.\n",
        "Answer: You ordered a Pepperoni pizza and a Coke. The total is $18.00.\n",
        "\n",
        "Example session 2:\n",
        "\n",
        "Question: I'll have a Cheese pizza and a Sprite. I'm eating here.\n",
        "Thought: I need to check prices.\n",
        "Action: get_menu\n",
        "PAUSE\n",
        "\n",
        "You will be called again with this:\n",
        "\n",
        "Observation: {\"pizzas\": {\"Pepperoni\": 15.00, \"Cheese\": 12.00}, \"sodas\": {\"Coke\": 3.00, \"Sprite\": 3.00}}\n",
        "\n",
        "Thought: Cheese is 12.00. Sprite is 3.00. The customer is \"eating here\", so I must add 10% to the total. Calculation is (12 + 3) * 1.10.\n",
        "Action: calculate: (12 + 3) * 1.10\n",
        "PAUSE\n",
        "\n",
        "You will be called again with this:\n",
        "\n",
        "Observation: 16.5\n",
        "\n",
        "Thought: The total includes the service fee.\n",
        "Answer: You ordered a Cheese pizza and a Sprite for dine-in. The total is $16.50.\n",
        "\n",
        "Now it's your turn:\n",
        "\n",
        "\"\"\".strip()"
      ],
      "metadata": {
        "id": "lhWp2Iqi3pbj"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementação das tools\n",
        "Segue implementação das tools definidas no system prompt. Aqui surgiu uma dificuldade de implementação porque suas assinaturas são diferentes, então precisam ser tratadas de maneira diferente."
      ],
      "metadata": {
        "id": "8NvJWtBYzZb6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tools\n",
        "\n",
        "\n",
        "import json\n",
        "\n",
        "def get_menu():\n",
        "    menu_data = {\n",
        "        \"pizzas\": {\n",
        "            \"Pepperoni\": 15.00,\n",
        "            \"Cheese\": 12.00\n",
        "        },\n",
        "        \"sodas\": {\n",
        "            \"Coke\": 3.00,\n",
        "            \"Sprite\": 3.00\n",
        "        }\n",
        "    }\n",
        "    return json.dumps(menu_data)\n",
        "\n",
        "\n",
        "def calculate(expression):\n",
        "    try:\n",
        "        expression = expression.strip()\n",
        "        allowed_names = {\"__builtins__\": None}\n",
        "        result = eval(expression, allowed_names)\n",
        "\n",
        "        return str(result)\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error: {str(e)}\""
      ],
      "metadata": {
        "id": "Wx4-JHQa6hKu"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Definição e execução do loop"
      ],
      "metadata": {
        "id": "iUejVph2zqiE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "agente = Agent(client=client, system=system_prompt)\n",
        "agente.loop(query=\"I want a Cheese pizza and a Coke. I will be eating at the restaurant.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FjHM0GNqlMwa",
        "outputId": "ff2dab8a-3ead-44e0-bd81-c6a3e9903c01"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Iteração 1 ---\n",
            "Thought: I need to check the prices for Cheese pizza and Coke. Since the customer is eating at the restaurant, I will also need to add a 10% service fee to the total.\n",
            "\n",
            "Action: get_menu\n",
            "PAUSE\n",
            "Observation: {\"pizzas\": {\"Pepperoni\": 15.0, \"Cheese\": 12.0}, \"sodas\": {\"Coke\": 3.0, \"Sprite\": 3.0}}\n",
            "\n",
            "--- Iteração 2 ---\n",
            "Thought: The price of a Cheese pizza is 12.0 and the price of a Coke is 3.0. Since the customer is eating at the restaurant, I need to calculate the total with a 10% service fee. The calculation is (12 + 3) * 1.10.\n",
            "\n",
            "Action: calculate: (12 + 3) * 1.10\n",
            "PAUSE\n",
            "Observation: 16.5\n",
            "\n",
            "--- Iteração 3 ---\n",
            "Thought: The calculation is complete. I have the final total, which includes the 10% service fee for dining in. The total is $16.50.\n",
            "\n",
            "Answer: You ordered a Cheese pizza and a Coke for dine-in. The total is $16.50.\n"
          ]
        }
      ]
    }
  ]
}